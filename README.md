# Multicore Processors

   Chip multiprocessor or multi-core processor is a single computing component with two or more independent processing units (cores), which is the units that read and deploys program instructions. The instructions usually commands that come from the CPU, these commands include add, branch and to move data. The benefits of multi-core is that it can run multiple instructions are the same time in-which can increase the overall speed in compliant with parallel computing. The development of an updated CPU in the computer system has remained stagnant and has remained single core over 30 year. The computer clock speed however, has increased (from a few megahertz to a few gigahertz) and the computer memory has expanded from kilobytes to gigabytes. Subsequently, in 2007 Intel released computers with dual core processor chips; they bundle two CPUs on a single slab of silicon. The two CPUs double the power of the machine by sharing the computation. Following the release of the dual core processors, quad-core and eight-core have been introduced. In the years comes, computers are expected to have hundreds or even thousands of processors. Although chipmakers are able to increase the processor, they have not been able to increase the speed of the CPU.<br>

   Gordon E. Moore, co-founder of Intel, forecasted the growth of transistors would double approximately every two years. Logic gates are made up of transistors, in which are fundamental to how processors compute. Manufacturers began to increase the amount of transistors that they placed on to the chip. This method of update did not increase the cost of the chip and as predicted the price of the transistors decreased as tine went on. For several decades his prediction proved accurate and Moore’s law was used to guide long-term planning in the computer industry. It was also noted that frequency increased 40% overtime in which caused the price per transistor continues to decrease over time. The upgrade to the transistors count was possible by increasing the amount of the amount of processing power into a single processing unit. The increase in the speed of the processor was possible by increasing the clock speed of an individual processor core, but when the maximum clock speed have been reached the power usage of a single processor would not stay the same as the clock speed.
“Scaling Laws”, first stated in 1974 by Robert H. Dennard and his colleagues at IBM, questioned the reduction of the size of a transistor and adjusting other factors to control its operation, such as voltage and currents. And seeing what changes could have an effect on performance. Dennard discovered when keeping voltage and current proportioned to the liner dimension of the device, which will keep power constant to the transistor. This will keep the power density constant, even if there is a device increase on the chip. He also discovered different delay on a single transistor was equal to its size so decreasing the size of individual transistors would mean increase in speed as well.<br>

   For decades, computer manufacturers were able to successfully increase the clock speed on a single processor. However, increasing the clock speed would cause the voltage to drop. When approaching certain voltage, the transistors would no longer be in compliance with the scaling laws in regards to power, causing a leakage in which will lead to high power consumption and heat. As time progressed, the conception of power has risen aggressively to the core performance, which means the amount heat that is being produced has became irrelevant, and it is no longer possible to increase clock speed and reduce the power per transistor.<br>

   As clock speed became a thousand times faster, the speed of the memory has only increased by around ten or so. This gap leads to inefficiency, the CPU would have to continue to cycle, as memory is slow to respond to request. Modern day processors could deploy hundreds of instructions while it takes the same amount of time for memory to fetch a data. One solution offered to fix this problem is to transfer the data from memory in blocks rather than single bits. This solution will improve throughput (the amount of work that a computer can do in a given time) but will not improve latency (the amount of time that it takes for the instruction to travel trough a system). To alleviate this problem with latency, modern day computers are equipped with a sophisticated echelon of cache memories in which are placed surrounding the processor core. The data’s and instructions are ordered based on usage. The fist-level cache (L1) is the fastest cache and it usually comes within the processor chip itself. The second-level cache (L2) which is larger and a bit slower than L1, L2 is equipped with information that slightly less urgent than L1.<br> 

   Third-level cache is rare in modern day computers; its functionality has been replaced by L2. Often in new computers L3 cache is located on the motherboard rather than the processor. It lives alongside the RAM and L2 cache. When fetching for data the system will begin looking for the data starting with L1, if the data is not found than it then it will check L2 and L3. If the data is not found within the cache memories than it will check RAM.
The power and energy concern continues to cause problems for chipmakers. As stated in Denard’s scaling laws, the power density would remain the same even when the number when increasing the number of transistors and their switching speed increase. The principle design prohibits functionality at lower operating voltages that steeply. A small amount of reduction in voltage could be problematic. As voltage is lowered, transistors tend to create a leakage. If the voltage is lower to a certain point, leakage could become unmanageable. But without reducing the voltage, clock rate could not be reduced. When the device structure size is below 65nm, the scaling rules are no longer valid due to the increase of the leakage in current. In the attempt to maintain the leakage current, Intel introduced a Hafnium based gate isolators for the Penryn processor. This new chip incorporating 45nm Hafnium-based high-k metal gate transistors that are smaller and more efficient than the previous generation. Replacing the silicon dioxide with thicker hafnium-based high-k metal reduced the leakage by more than ten times when comparing it to the silicon dioxide used for more that four decades. This is the biggest breakthrough for transistor advancement in over 40 years.<br>

   Commercial incentives motivate the development of the multi-core architecture. Since the late 1960s, it was possible to enhance the performance of the CPU by shrinking the area of the integrated circuit, which decreased the price on the integrated circuit. With in this area, additional transistors could be implanted to increase functionality. The multicore processors that have been introduced to the market by Intel and AMD are dual and quad core processors that provide dual threading to increase the parallelization within a single core. The dual core CPUs holds two execution cores in one integrated circuit, which preform faster than the single core processors. These cores act as a single unit and are provided with their own controller and cache. The quad-core CPU design is a bit more complicated than the dual-core CPUs. For example, in some cases chips may not share resources like caches.<br>
